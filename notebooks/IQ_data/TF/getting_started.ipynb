{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tied EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 59.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Class Probabilities\n",
      "tf.Tensor([0.30003592 0.6999643 ], shape=(2,), dtype=float32)\n",
      "------\n",
      "Mus\n",
      "tf.Tensor(\n",
      "[[ 5.0032625  5.0041113]\n",
      " [-2.991531  -1.9996748]], shape=(2, 2), dtype=float32)\n",
      "------\n",
      "Covariance Matrices\n",
      "tf.Tensor(\n",
      "[[[1.5098091  0.50375587]\n",
      "  [0.5037559  1.9828072 ]]\n",
      "\n",
      " [[1.4781156  0.00526009]\n",
      "  [0.00526009 1.7913566 ]]], shape=(2, 2, 2), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def em(dataset, n_clusters, n_iter=100):\n",
    "    # Infer from the dataset\n",
    "    n_samples, n_dims = dataset.shape\n",
    "\n",
    "    # Draw initial guesses\n",
    "    cluster_probs = tfp.distributions.Dirichlet(tf.ones(n_clusters)).sample(seed=42)\n",
    "    mus = tfp.distributions.Normal(loc=0.0, scale=3.0).sample((n_clusters, n_dims), seed=42)\n",
    "    covs = tfp.distributions.WishartTriL(df=3, scale_tril=tf.eye(n_dims)).sample(n_clusters, seed=42)\n",
    "\n",
    "    for _ in tqdm(range(n_iter)):\n",
    "        # Batched Cholesky Factorization\n",
    "        Ls = tf.linalg.cholesky(covs)\n",
    "        normals = tfp.distributions.MultivariateNormalTriL(\n",
    "            loc=mus,\n",
    "            scale_tril=Ls\n",
    "        )\n",
    "\n",
    "\n",
    "        ### E-Step\n",
    "\n",
    "        # (1) resp is of shape (n_samples x n_clusters)\n",
    "        # batched multivariate normal is of shape (n_clusters x n_dims)\n",
    "        unnormalized_responsibilities = (\n",
    "            tf.reshape(cluster_probs, (1, n_clusters)) * normals.prob(tf.reshape(dataset, (n_samples, 1, n_dims)))\n",
    "        )\n",
    "\n",
    "        # (2)\n",
    "        responsibilities = unnormalized_responsibilities / tf.reduce_sum(unnormalized_responsibilities, axis=1, keepdims=True)\n",
    "\n",
    "        # (3)\n",
    "        class_responsibilities = tf.reduce_sum(responsibilities, axis=0)\n",
    "\n",
    "        ### M-Step\n",
    "\n",
    "        # (1)\n",
    "        cluster_probs = class_responsibilities / n_samples\n",
    "\n",
    "        # (2)\n",
    "        # class_responsibilities is of shape (n_clusters)\n",
    "        # responsibilities is of shape (n_samples, n_clusters)\n",
    "        # dataset is of shape (n_samples, n_dims)\n",
    "        #\n",
    "        # mus is of shape (n_clusters, n_dims)\n",
    "        #\n",
    "        # -> summation has to occur over the samples axis\n",
    "        mus = tf.reduce_sum(\n",
    "            tf.reshape(responsibilities, (n_samples, n_clusters, 1)) * tf.reshape(dataset, (n_samples, 1, n_dims)),\n",
    "            axis=0,\n",
    "        ) / tf.reshape(class_responsibilities, (n_clusters, 1))\n",
    "        \n",
    "        # (3)\n",
    "        # class_responsibilities is of shape (n_clusters)\n",
    "        # dataset is of shape (n_samples, n_dims)\n",
    "        # mus is of shape (n_clusters, n_dims)\n",
    "        # responsibilities is of shape (n_samples, n_clusters)\n",
    "        #\n",
    "        # covs is of shape (n_clusters, n_dims, n_dims)\n",
    "        \n",
    "        # (n_clusters, n_samples, n_dims)\n",
    "        centered_datasets = tf.reshape(dataset, (1, n_samples, n_dims)) - tf.reshape(mus, (n_clusters, 1, n_dims))\n",
    "        centered_datasets_with_responsibilities = centered_datasets * tf.reshape(tf.transpose(responsibilities), (n_clusters, n_samples, 1))\n",
    "        \n",
    "        # Batched Matrix Multiplication\n",
    "        # (n_clusters, n_dims, n_dims)\n",
    "        sample_covs = tf.matmul(centered_datasets_with_responsibilities, centered_datasets, transpose_a=True)\n",
    "\n",
    "        covs = sample_covs / tf.reshape(class_responsibilities, (n_clusters, 1, 1))\n",
    "\n",
    "\n",
    "        # Ensure positive definiteness by adding a \"small amount\" to the diagonal\n",
    "        covs = covs + 1.0e-8 * tf.eye(n_dims, batch_shape=(n_clusters, ))\n",
    "\n",
    "    \n",
    "    return cluster_probs, mus, covs\n",
    "\n",
    "\n",
    "def main():\n",
    "    N_CLUSTERS = 2\n",
    "    CLUSTER_PROBS = [0.3, 0.7]\n",
    "    MUS_TRUE = [\n",
    "        [5.0, 5.0],\n",
    "        [-3.0, -2.0],\n",
    "    ]\n",
    "    COVS_TRUE = [\n",
    "        [\n",
    "            [1.5, 0.5],\n",
    "            [0.5, 2.0],\n",
    "        ],\n",
    "        [\n",
    "            [1.5, 0.0],\n",
    "            [0.0, 1.8],\n",
    "        ]\n",
    "    ]\n",
    "    N_SAMPLES = 60000\n",
    "\n",
    "    # Batched Cholesky factorization of the covariance matrices\n",
    "    LS_TRUE = tf.linalg.cholesky(COVS_TRUE)\n",
    "\n",
    "    # The true Gaussian Mixture Model (we want to use for sampling some\n",
    "    # artificial data)\n",
    "    cat = tfp.distributions.Categorical(\n",
    "        probs=CLUSTER_PROBS,\n",
    "    )\n",
    "    normals = tfp.distributions.MultivariateNormalTriL(\n",
    "        loc=MUS_TRUE,\n",
    "        scale_tril=LS_TRUE,\n",
    "    )\n",
    "\n",
    "    gmm_true = tfp.distributions.MixtureSameFamily(\n",
    "        mixture_distribution=cat,\n",
    "        components_distribution=normals,\n",
    "    )\n",
    "\n",
    "    dataset = gmm_true.sample(N_SAMPLES, seed=42)\n",
    "\n",
    "    # print(dataset)\n",
    "    # plt.scatter(dataset.numpy()[:, 0], dataset.numpy()[:, 1])\n",
    "    # plt.show()\n",
    "\n",
    "    class_probs_approx, mus_approx, covs_approx = em(dataset, N_CLUSTERS)\n",
    "    \n",
    "    print(\"------\")\n",
    "    print(\"Class Probabilities\")\n",
    "    print(class_probs_approx)\n",
    "    print(\"------\")\n",
    "    print(\"Mus\")\n",
    "    print(mus_approx)\n",
    "    print(\"------\")\n",
    "    print(\"Covariance Matrices\")\n",
    "    print(covs_approx)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Soft-Info-fMUpUe5a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
